<style>
/* Styles pour les cours avec syst√®me de cartes */

.course-container {
    display: flex;
    flex-direction: column;
    gap: 1.5rem;
    padding: 1rem 0;
    max-width: 100%;
}

.course-card {
    background: var(--md-default-bg-color);
    border-radius: 12px;
    padding: 1.5rem;
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    transition: transform 0.3s ease, box-shadow 0.3s ease;
    border-left: 4px solid;
    width: 100%;
    max-width: 100%;
    margin: 1rem 0;
}

.course-card.definition {
    border-left-color: #4CAF50;
    background: linear-gradient(135deg, rgba(76, 175, 80, 0.05) 0%, rgba(76, 175, 80, 0.02) 100%);
}

.course-card.definition:hover {
    transform: translateY(-3px);
    box-shadow: 0 0 20px rgba(76, 175, 80, 0.3);
}

.course-card.example {
    border-left-color: #2196F3;
    background: linear-gradient(135deg, rgba(33, 150, 243, 0.05) 0%, rgba(33, 150, 243, 0.02) 100%);
}

.course-card.example:hover {
    transform: translateY(-3px);
    box-shadow: 0 0 20px rgba(33, 150, 243, 0.3);
}

.course-card.warning {
    border-left-color: #F44336;
    background: linear-gradient(135deg, rgba(244, 67, 54, 0.05) 0%, rgba(244, 67, 54, 0.02) 100%);
}

.course-card.warning:hover {
    transform: translateY(-3px);
    box-shadow: 0 0 20px rgba(244, 67, 54, 0.3);
}

.course-card.tip {
    border-left-color: #FF9800;
    background: linear-gradient(135deg, rgba(255, 152, 0, 0.05) 0%, rgba(255, 152, 0, 0.02) 100%);
}

.course-card.tip:hover {
    transform: translateY(-3px);
    box-shadow: 0 0 20px rgba(255, 152, 0, 0.3);
}

.course-card.highlight {
    border-left-color: #9C27B0;
    background: linear-gradient(135deg, rgba(156, 39, 176, 0.05) 0%, rgba(156, 39, 176, 0.02) 100%);
}

.course-card.highlight:hover {
    transform: translateY(-3px);
    box-shadow: 0 0 20px rgba(156, 39, 176, 0.3);
}

.course-title {
    margin: 0 0 1rem 0;
    color: var(--md-primary-fg-color);
    font-size: 1.3rem;
    font-weight: 700;
    display: flex;
    align-items: center;
    gap: 0.5rem;
}

.course-content {
    margin-bottom: 1rem;
    line-height: 1.7;
    font-size: 1rem;
}

.badge {
    display: inline-block;
    padding: 0.3rem 0.8rem;
    border-radius: 15px;
    font-size: 0.85rem;
    font-weight: 600;
    margin-bottom: 0.8rem;
}

.badge.definition {
    background: rgba(76, 175, 80, 0.15);
    color: #4CAF50;
}

.badge.example {
    background: rgba(33, 150, 243, 0.15);
    color: #2196F3;
}

.badge.warning {
    background: rgba(244, 67, 54, 0.15);
    color: #F44336;
}

.badge.tip {
    background: rgba(255, 152, 0, 0.15);
    color: #FF9800;
}

.badge.highlight {
    background: rgba(156, 39, 176, 0.15);
    color: #9C27B0;
}

.btn {
    background: white;
    color: #4169E1;
    border: 2px solid #4169E1;
    padding: 0.8rem 1.5rem;
    border-radius: 10px;
    cursor: pointer;
    font-size: 1rem;
    font-weight: 600;
    transition: all 0.3s ease;
    display: inline-flex;
    align-items: center;
    gap: 0.5rem;
    margin-top: 1rem;
    text-decoration: none;
}

.btn:hover {
    background: rgba(65, 105, 225, 0.1);
    border-color: #1E90FF;
    color: #1E90FF;
    transform: translateY(-2px);
    box-shadow: 0 6px 15px rgba(65, 105, 225, 0.4);
}

.exercise-container {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 2rem;
    border-radius: 15px;
    margin: 2rem 0;
    text-align: center;
    box-shadow: 0 8px 25px rgba(102, 126, 234, 0.3);
}

.math-container {
    background: #f8f9fa;
    border: 1px solid #e9ecef;
    border-radius: 8px;
    padding: 1.5rem;
    margin: 1.5rem 0;
    text-align: center;
}

.table {
    width: 100%;
    border-collapse: collapse;
    margin: 1.5rem 0;
    background: white;
    border-radius: 8px;
    overflow: hidden;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
}

.table th,
.table td {
    padding: 1rem;
    text-align: left;
    border-bottom: 1px solid #e9ecef;
}

.table th {
    background: #f8f9fa;
    font-weight: 600;
    color: #495057;
}

.table tr:hover {
    background: #f8f9fa;
}

code {
    background: #f1f3f4;
    padding: 0.2rem 0.4rem;
    border-radius: 4px;
    font-family: 'Courier New', monospace;
    font-size: 0.9rem;
    color: #d63384;
}

pre {
    background: #f8f9fa;
    border: 1px solid #e9ecef;
    border-radius: 8px;
    padding: 1.5rem;
    overflow-x: auto;
    margin: 1.5rem 0;
}

pre code {
    background: none;
    padding: 0;
    color: #495057;
}

.highlight {
    background: linear-gradient(120deg, #a8edea 0%, #fed6e3 100%);
    padding: 0.2rem 0.5rem;
    border-radius: 4px;
    font-weight: 600;
}
</style>

# üìö Fiche de Cours : Algorithme des K Plus Proches Voisins (KNN)

<div class="course-container">
    <div class="course-card definition">
        <div class="badge definition">üìñ D√©finition</div>
        <h3 class="course-title">1. Qu'est-ce que l'algorithme des K Plus Proches Voisins (KNN) ?</h3>
        <div class="course-content">
            L'algorithme des K Plus Proches Voisins (K-Nearest Neighbors ou KNN) est une m√©thode d'<strong>apprentissage supervis√©</strong> simple et intuitive, utilis√©e principalement pour des probl√®mes de <strong>classification</strong> et de <strong>r√©gression</strong>.<br><br>*   <strong>Apprentissage supervis√©</strong> : Cela signifie que l'algorithme apprend √† partir d'un ensemble de donn√©es o√π les "bonnes r√©ponses" (√©tiquettes ou valeurs) sont d√©j√† connues.<br>*   <strong>Classification</strong> : Pr√©dire √† quelle cat√©gorie appartient un nouvel √©l√©ment (par exemple, si un email est un spam ou non).<br>*   <strong>R√©gression</strong> : Pr√©dire une valeur continue pour un nouvel √©l√©ment (par exemple, estimer le prix d'une maison).<br><br>L'id√©e centrale de KNN est que des choses similaires existent √† proximit√© les unes des autres. Autrement dit, un nouvel √©l√©ment sera probablement similaire √† ses voisins les plus proches dans l'ensemble de donn√©es d'entra√Ænement.
        </div>
    </div>
    
    <div class="course-card definition">
        <div class="badge definition">üìñ D√©finition</div>
        <h3 class="course-title">2. Comment fonctionne KNN ?</h3>
        <div class="course-content">
            L'algorithme KNN fonctionne en plusieurs √©tapes :<br><br>1.  <strong>Choisir le nombre K</strong> : On d√©finit K, le nombre de voisins les plus proches √† consid√©rer. C'est un param√®tre important √† r√©gler.<br>2.  <strong>Calculer les distances</strong> : Pour un nouvel √©l√©ment que l'on souhaite classifier (ou pour lequel on veut pr√©dire une valeur), on calcule la distance entre cet √©l√©ment et tous les autres √©l√©ments de l'ensemble de donn√©es d'entra√Ænement. La mesure de distance la plus courante est la <strong>distance euclidienne</strong>.<br>3.  <strong>Identifier les K voisins</strong> : On s√©lectionne les K √©l√©ments de l'ensemble d'entra√Ænement qui sont les plus proches du nouvel √©l√©ment (ceux ayant les plus petites distances).<br>4.  <strong>Prendre une d√©cision</strong> :<br>    *   <strong>Pour la classification</strong> : On regarde la cat√©gorie majoritaire parmi les K voisins. Le nouvel √©l√©ment est assign√© √† cette cat√©gorie.<br>    *   <strong>Pour la r√©gression</strong> : On calcule la moyenne (ou parfois la m√©diane) des valeurs des K voisins. Cette moyenne devient la valeur pr√©dite pour le nouvel √©l√©ment.<br><br>### Exemple simple de classification :<br><br>Imaginons que nous ayons des points de donn√©es de deux cat√©gories (A et B) et un nouveau point inconnu (X). Si nous choisissons K=3 :<br><br>1.  On calcule la distance entre X et tous les autres points.<br>2.  On trouve les 3 points les plus proches de X.<br>3.  Si 2 de ces 3 voisins sont de la cat√©gorie A et 1 est de la cat√©gorie B, alors X sera classifi√© comme A.
        </div>
    </div>
    
    <div class="course-card definition">
        <div class="badge definition">üìñ D√©finition</div>
        <h3 class="course-title">3. La mesure de distance</h3>
        <div class="course-content">
            La <strong>distance euclidienne</strong> est la plus utilis√©e. Pour deux points P(p1, p2, ..., pn) et Q(q1, q2, ..., qn) dans un espace √† n dimensions, la distance euclidienne est :<br><br><code>d(P, Q) = ‚àö((p1-q1)¬≤ + (p2-q2)¬≤ + ... + (pn-qn)¬≤) </code><br><br>D'autres mesures de distance existent, comme la distance de Manhattan ou la distance de Minkowski.<br><br><strong>Important : La normalisation des donn√©es</strong><br>Si les diff√©rentes caract√©ristiques (dimensions) de vos donn√©es n'ont pas la m√™me √©chelle (par exemple, une caract√©ristique varie de 0 √† 1 et une autre de 0 √† 1000), celles avec des valeurs plus grandes domineront le calcul de la distance. Il est donc crucial de <strong>normaliser</strong> ou <strong>standardiser</strong> vos donn√©es avant d'appliquer KNN.
        </div>
    </div>
    
    <div class="course-card definition">
        <div class="badge definition">üìñ D√©finition</div>
        <h3 class="course-title">4. Choisir la valeur de K</h3>
        <div class="course-content">
            Le choix de K est crucial :<br><br>*   <strong>K petit</strong> (par exemple, K=1) : L'algorithme est tr√®s sensible au bruit et aux points aberrants. Le mod√®le peut √™tre trop sp√©cifique aux donn√©es d'entra√Ænement (surapprentissage ou overfitting).<br>*   <strong>K grand</strong> : L'algorithme est plus robuste au bruit, mais peut rendre les fronti√®res de d√©cision moins distinctes. Si K est trop grand (par exemple, K = nombre total d'√©chantillons), tous les nouveaux points seront classifi√©s dans la cat√©gorie majoritaire de l'ensemble d'entra√Ænement.<br><br>Il n'y a pas de r√®gle d'or pour choisir K. On utilise souvent des techniques comme la validation crois√©e pour trouver une valeur optimale de K pour un probl√®me donn√©. Une pratique courante est de choisir K comme un nombre impair pour √©viter les √©galit√©s en classification binaire.
        </div>
    </div>
    
    <div class="course-card definition">
        <div class="badge definition">üìñ D√©finition</div>
        <h3 class="course-title">5. Avantages de KNN</h3>
        <div class="course-content">
            *   <strong>Simple √† comprendre et √† impl√©menter</strong>.<br>*   <strong>Non param√©trique</strong> : Il ne fait aucune supposition sur la distribution sous-jacente des donn√©es.<br>*   <strong>Polyvalent</strong> : Peut √™tre utilis√© pour la classification et la r√©gression.<br>*   <strong>S'adapte facilement</strong> : De nouvelles donn√©es peuvent √™tre ajout√©es sans avoir √† r√©-entra√Æner le mod√®le (on parle d'apprentissage "paresseux" ou "lazy learning" car la majorit√© du calcul se fait au moment de la pr√©diction).
        </div>
    </div>
    
    <div class="course-card definition">
        <div class="badge definition">üìñ D√©finition</div>
        <h3 class="course-title">6. Inconv√©nients de KNN</h3>
        <div class="course-content">
            *   <strong>Co√ªteux en calcul</strong> : Pour chaque nouvelle pr√©diction, il faut calculer la distance √† tous les points de l'ensemble d'entra√Ænement. Cela peut √™tre tr√®s lent pour de grands ensembles de donn√©es.<br>*   <strong>Sensible aux dimensions non pertinentes</strong> (le "fl√©au de la dimensionnalit√©") : Si l'ensemble de donn√©es a beaucoup de caract√©ristiques (dimensions) et que la plupart ne sont pas informatives, la performance de KNN peut se d√©grader.<br>*   <strong>N√©cessite une normalisation des donn√©es</strong> : Comme mentionn√©, les √©chelles des caract√©ristiques influencent fortement les distances.<br>*   <strong>N√©cessite beaucoup de m√©moire</strong> : Il faut stocker tout l'ensemble de donn√©es d'entra√Ænement.<br>*   <strong>Le choix de K est critique</strong>.
        </div>
    </div>
    
    <div class="course-card definition">
        <div class="badge definition">üìñ D√©finition</div>
        <h3 class="course-title">7. Quand utiliser KNN ?</h3>
        <div class="course-content">
            KNN peut √™tre un bon point de d√©part pour des probl√®mes de classification ou de r√©gression, surtout si :<br><br>*   L'ensemble de donn√©es n'est pas excessivement grand.<br>*   Les relations entre les caract√©ristiques et la cible sont complexes et non lin√©aires.<br>*   Vous avez besoin d'une m√©thode simple et interpr√©table.<br><br>Il est souvent utilis√© comme baseline pour comparer avec des mod√®les plus complexes.
        </div>
    </div>
    
</div>