# üìö Fiche de Cours : Algorithme des K Plus Proches Voisins (KNN)

## üìñ D√©finition

### 1. Qu'est-ce que l'algorithme des K Plus Proches Voisins (KNN) ?

L'algorithme des K Plus Proches Voisins (K-Nearest Neighbors ou KNN) est une m√©thode d'**apprentissage supervis√©** simple et intuitive, utilis√©e principalement pour des probl√®mes de **classification** et de **r√©gression**.

- **Apprentissage supervis√©** : Cela signifie que l'algorithme apprend √† partir d'un ensemble de donn√©es o√π les "bonnes r√©ponses" (√©tiquettes ou valeurs) sont d√©j√† connues.
- **Classification** : Pr√©dire √† quelle cat√©gorie appartient un nouvel √©l√©ment (par exemple, si un email est un spam ou non).
- **R√©gression** : Pr√©dire une valeur continue pour un nouvel √©l√©ment (par exemple, estimer le prix d'une maison).

L'id√©e centrale de KNN est que des choses similaires existent √† proximit√© les unes des autres. Autrement dit, un nouvel √©l√©ment sera probablement similaire √† ses voisins les plus proches dans l'ensemble de donn√©es d'entra√Ænement.

### 2. Comment fonctionne KNN ?

L'algorithme KNN fonctionne en plusieurs √©tapes :

1. **Choisir le nombre K** : On d√©finit K, le nombre de voisins les plus proches √† consid√©rer. C'est un param√®tre important √† r√©gler.
2. **Calculer les distances** : Pour un nouvel √©l√©ment que l'on souhaite classifier (ou pour lequel on veut pr√©dire une valeur), on calcule la distance entre cet √©l√©ment et tous les autres √©l√©ments de l'ensemble de donn√©es d'entra√Ænement. La mesure de distance la plus courante est la **distance euclidienne**.
3. **Identifier les K voisins** : On s√©lectionne les K √©l√©ments de l'ensemble d'entra√Ænement qui sont les plus proches du nouvel √©l√©ment (ceux ayant les plus petites distances).
4. **Prendre une d√©cision** :
   - **Pour la classification** : On regarde la cat√©gorie majoritaire parmi les K voisins. Le nouvel √©l√©ment est assign√© √† cette cat√©gorie.
   - **Pour la r√©gression** : On calcule la moyenne (ou parfois la m√©diane) des valeurs des K voisins. Cette moyenne devient la valeur pr√©dite pour le nouvel √©l√©ment.

#### Exemple simple de classification :

Imaginons que nous ayons des points de donn√©es de deux cat√©gories (A et B) et un nouveau point inconnu (X). Si nous choisissons K=3 :

1. On calcule la distance entre X et tous les autres points.
2. On trouve les 3 points les plus proches de X.
3. Si 2 de ces 3 voisins sont de la cat√©gorie A et 1 est de la cat√©gorie B, alors X sera classifi√© comme A.

### 3. La mesure de distance

La **distance euclidienne** est la plus utilis√©e. Pour deux points P(p1, p2, ..., pn) et Q(q1, q2, ..., qn) dans un espace √† n dimensions, la distance euclidienne est :

```
d(P, Q) = ‚àö((p1-q1)¬≤ + (p2-q2)¬≤ + ... + (pn-qn)¬≤)
```

D'autres mesures de distance existent, comme la distance de Manhattan ou la distance de Minkowski.

**Important : La normalisation des donn√©es**

Si les diff√©rentes caract√©ristiques (dimensions) de vos donn√©es n'ont pas la m√™me √©chelle (par exemple, une caract√©ristique varie de 0 √† 1 et une autre de 0 √† 1000), celles avec des valeurs plus grandes domineront le calcul de la distance. Il est donc crucial de **normaliser** ou **standardiser** vos donn√©es avant d'appliquer KNN.

### 4. Choisir la valeur de K

Le choix de K est crucial :

- **K petit** (par exemple, K=1) : L'algorithme est tr√®s sensible au bruit et aux points aberrants. Le mod√®le peut √™tre trop sp√©cifique aux donn√©es d'entra√Ænement (surapprentissage ou overfitting).
- **K grand** : L'algorithme est plus robuste au bruit, mais peut rendre les fronti√®res de d√©cision moins distinctes. Si K est trop grand (par exemple, K = nombre total d'√©chantillons), tous les nouveaux points seront classifi√©s dans la cat√©gorie majoritaire de l'ensemble d'entra√Ænement.

Il n'y a pas de r√®gle d'or pour choisir K. On utilise souvent des techniques comme la validation crois√©e pour trouver une valeur optimale de K pour un probl√®me donn√©. Une pratique courante est de choisir K comme un nombre impair pour √©viter les √©galit√©s en classification binaire.

### 5. Avantages de KNN

- **Simple √† comprendre et √† impl√©menter**.
- **Non param√©trique** : Il ne fait aucune supposition sur la distribution sous-jacente des donn√©es.
- **Polyvalent** : Peut √™tre utilis√© pour la classification et la r√©gression.
- **S'adapte facilement** : De nouvelles donn√©es peuvent √™tre ajout√©es sans avoir √† r√©-entra√Æner le mod√®le (on parle d'apprentissage "paresseux" ou "lazy learning" car la majorit√© du calcul se fait au moment de la pr√©diction).

### 6. Inconv√©nients de KNN

- **Co√ªteux en calcul** : Pour chaque nouvelle pr√©diction, il faut calculer la distance √† tous les points de l'ensemble d'entra√Ænement. Cela peut √™tre tr√®s lent pour de grands ensembles de donn√©es.
- **Sensible aux dimensions non pertinentes** (le "fl√©au de la dimensionnalit√©") : Si l'ensemble de donn√©es a beaucoup de caract√©ristiques (dimensions) et que la plupart ne sont pas informatives, la performance de KNN peut se d√©grader.
- **N√©cessite une normalisation des donn√©es** : Comme mentionn√©, les √©chelles des caract√©ristiques influencent fortement les distances.
- **N√©cessite beaucoup de m√©moire** : Il faut stocker tout l'ensemble de donn√©es d'entra√Ænement.
- **Le choix de K est critique**.

### 7. Quand utiliser KNN ?

KNN peut √™tre un bon point de d√©part pour des probl√®mes de classification ou de r√©gression, surtout si :

- L'ensemble de donn√©es n'est pas excessivement grand.
- Les relations entre les caract√©ristiques et la cible sont complexes et non lin√©aires.
- Vous avez besoin d'une m√©thode simple et interpr√©table.

Il est souvent utilis√© comme baseline pour comparer avec des mod√®les plus complexes.